{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='./assets/jWST-SS-1300x300_Banner2.jpg' width=\"1000px\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "import os,shutil,glob,sys,traceback\n",
    "from astropy.cosmology import LambdaCDM\n",
    "import pdb\n",
    "import pickle\n",
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "import jhat\n",
    "from jhat import jwst_photclass,hst_photclass,st_wcs_align\n",
    "import space_phot\n",
    "import astropy\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astropy.time import Time\n",
    "from jwst.pipeline import Image3Pipeline\n",
    "import pprint\n",
    "from astroquery.mast import Observations\n",
    "import subprocess\n",
    "import sewpy\n",
    "from astropy.wcs import WCS\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Summer School Week 2\n",
    "## Difference Imaging\n",
    "\n",
    "### Author: David Coulter (JHU/STScI), Justin Pierel (STScI), Mike Engesser (STScI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "**Purpose**:<BR>\n",
    "This notebook will cover a basic workflow for difference imaging for the purpose of discovering transients, and to enable photometry measurements on the resulting sources.\n",
    "\n",
    "**Data**:<BR>\n",
    "This example is set up to use an example dataset from the JADES program, PID 1180. Please see the JADES data download notebook for the astroquery code to download the specific files for this exercise from MAST, or you can retrieve the data from Box: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**Additional Dependency Install**:<BR>\n",
    "We need to install both Source Extractor (SE) and Sewpy, a Python wrapper for SE. We use conda-forge to install a pre-compiled SE library, and then install Sewpy from source.<BR>\n",
    "\n",
    "[Sewpy](https://sewpy.readthedocs.io/en/latest/installation.html)<br>\n",
    "[SExtractor](https://sextractor.readthedocs.io/en/latest/Using.html)\n",
    "Better [Documentation](https://astro.dur.ac.uk/~pdraper/extractor/Guide2source_extractor.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y conda-forge::astromatic-source-extractor\n",
    "!pip install git+https://github.com/megalut/sewpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Section 1: Aligning Lvl2 Cals and Preparing to Build Mosaics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Inputs: \n",
    "* Raw Lvl 2 Cals for each epoch\n",
    "   * F200W, F277W\n",
    "* Reference catalog for alignment in the field\n",
    "   * JADES [HLSP](https://jades-survey.github.io/)\n",
    "### Outputs:\n",
    "* Aligned Lvl 2 Cals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get supernova positions\n",
    "# https://www.wis-tns.org/object/2023adta\n",
    "SN2023adta = SkyCoord(53.135283, -27.814517, unit=(u.deg, u.deg), frame='icrs')\n",
    "\n",
    "# https://www.wis-tns.org/object/2023adsy\n",
    "SN2023adsy = SkyCoord(53.134853, -27.820899, unit=(u.deg, u.deg), frame='icrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### For convenience, let's arrange files into difference epochs to make processing more straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get epoch 1 and epoch 2 cals in an array\n",
    "epoch1_F356W_files = glob.glob(\"./JADES_data/epoch1/*nrcblong_cal.fits\")\n",
    "epoch1_F200W_files = glob.glob(\"./JADES_data/epoch1/*nrcb2_cal.fits\")\n",
    "epoch1_files = [] + epoch1_F356W_files + epoch1_F200W_files\n",
    "\n",
    "epoch2_F356W_files = glob.glob(\"./JADES_data/epoch2/*nrcblong_cal.fits\")\n",
    "epoch2_F200W_files = glob.glob(\"./JADES_data/epoch2/*nrcb2_cal.fits\")\n",
    "epoch2_files = [] + epoch2_F356W_files + epoch2_F200W_files\n",
    "\n",
    "# Make output dirs\n",
    "jhat_epoch1_outsubdir = \"epoch1_aligned\"\n",
    "jhat_epoch2_outsubdir = \"epoch2_aligned\"\n",
    "os.makedirs(jhat_epoch1_outsubdir, exist_ok=True)\n",
    "os.makedirs(jhat_epoch2_outsubdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Now, let's grab the JADES photometric catalog -- we will use this for alignment and for passing sources to our convolution ([HOTPANTS.](https://github.com/acbecker/hotpants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = \"https://archive.stsci.edu/hlsps/jades/dr2/goods-s/catalogs/hlsp_jades_jwst_nircam_goods-s-deep_photometry_v2.0_catalog.fits\"\n",
    "output_dir = \"./JADES_data\"\n",
    "filename = catalog_url.split('/')[-1]\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "r = requests.get(catalog_url, stream=True)\n",
    "with open(file_path, 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Build the catalog by converting fluxes to mags. See catalog documentation [here.](https://archive.stsci.edu/hlsps/jades/hlsp_jades_jwst_nircam_goods-s-deep_photometry_v2.0_catalog-ext-readme.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a magnitude ranked input catalog for JHAT from the JADES photometric catalog we just downloaded.\n",
    "ref_fits = \"./JADES_data/hlsp_jades_jwst_nircam_goods-s-deep_photometry_v2.0_catalog.fits\"\n",
    "ref_catname = \"./JADES_data/jhat_cat.txt\"\n",
    "max_depth = 28.0\n",
    "flux_units = astropy.units.nJy\n",
    "\n",
    "with fits.open(ref_fits, memmap=True) as cat_file:\n",
    "\n",
    "    table = Table(cat_file[\"CIRC_BSUB\"].data)\n",
    "\n",
    "    filt_name = \"F150W\"\n",
    "    flux_key = \"{filt}_CIRC0\".format(filt=filt_name)\n",
    "    err_key = \"{filt}_CIRC0_e\".format(filt=filt_name)\n",
    "\n",
    "    src_id = table[\"ID\"]\n",
    "    ra = table[\"RA\"]\n",
    "    dec = table[\"DEC\"]\n",
    "    nJy = table[flux_key]\n",
    "    nJy_err = table[err_key]\n",
    "\n",
    "    flux = nJy*flux_units\n",
    "    flux_err = nJy_err*flux_units\n",
    "    mag = flux.to(astropy.units.ABmag).value\n",
    "    mag_err = flux_err.to(astropy.units.ABmag).value\n",
    "    \n",
    "    output_tbl = Table([src_id, ra, dec, mag, mag_err], names=[\"ID\", \"RA\", \"DEC\", \"MAG\", \"MAG_ERR\"])\n",
    "\n",
    "    output_tbl = output_tbl[output_tbl[\"MAG\"] < max_depth]\n",
    "    output_tbl.write(ref_catname, format=\"ascii\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a utility function to allow us to visualize things in DS9\n",
    "def create_pixregionfile(x,y,regionname,color,coords='image',radius=1):\n",
    "    if isinstance(radius,int):\n",
    "        radius = [radius]*len(x)\n",
    "    with open(regionname, 'w') as f:\n",
    "        if isinstance(color,str):\n",
    "            f.write('global color={0} dashlist=8 3 width=2 font=\\\"helvetica 10 normal roman\\\" select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1 \\n'.format(color))\n",
    "            do_col = False\n",
    "        else:\n",
    "            do_col = True\n",
    "            f.write('global dashlist=8 3 width=2 font=\\\"helvetica 10 normal roman\\\" select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1 \\n')\n",
    "        f.write('%s \\n'%coords)\n",
    "        for star in range(len(x)):\n",
    "            xval = x[star]\n",
    "            yval = y[star]\n",
    "            if do_col:\n",
    "                f.write('circle({ra},{dec},{radius}\") # color={color}\\n'.format(ra=xval, dec=yval,radius=radius[star],color=color[star]))\n",
    "            else:\n",
    "                f.write('circle({ra},{dec},{radius}\")\\n'.format(ra=xval, dec=yval,radius=radius[star]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function wrapper for JHAT\n",
    "def align_and_create_region_files(input_file, output_subdir, reference_cat_name, max_depth):\n",
    "    \n",
    "    wcs_align = st_wcs_align()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\t***** Processing File: `%s` *****\" % f)\n",
    "    \n",
    "        wcs_align.run_all(input_file,\n",
    "                      telescope='jwst',\n",
    "                      outsubdir=output_subdir,\n",
    "                      refcat_racol='RA',\n",
    "                      refcat_deccol='DEC',\n",
    "                      refcat_magcol='MAG',\n",
    "                      refcat_magerrcol='MAG_ERR',\n",
    "                      overwrite=True,\n",
    "                      d2d_max=0.25,\n",
    "                      showplots=0,\n",
    "                      refcatname=reference_cat_name,\n",
    "                      histocut_order='dxdy',\n",
    "                      sharpness_lim=(0.3,0.9), # 0 to 1\n",
    "                      roundness1_lim=(-0.7, 0.7), # -1 to 1\n",
    "                      SNR_min= 3,\n",
    "                      dmag_max=1.0,\n",
    "                      objmag_lim =(14,max_depth),\n",
    "                      saveplots=2,\n",
    "                      savephottable=True)\n",
    "    \n",
    "        good_matches_file_name = os.path.basename(f).replace('_cal.fits', '.goodmatches.csv')\n",
    "        good_matches_file_path = \"./{base_path}/{file_name}\".format(base_path=jhat_epoch1_outsubdir, file_name=good_matches_file_name)\n",
    "        good_match_tbl = Table.read(good_matches_file_path, format=\"ascii\")\n",
    "\n",
    "        create_pixregionfile(good_match_tbl[\"RA\"], \n",
    "                             good_match_tbl[\"DEC\"],\n",
    "                             good_matches_file_path.replace(\"csv\", \"reg\"), \n",
    "                             color=\"red\", coords=\"icrs\", radius=[0.4]*len(good_match_tbl))\n",
    "        \n",
    "              \n",
    "    except Exception as e:\n",
    "        print(\"\\t\\t***** FAILED Processing File: `%s` *****\" % f)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### OK, now we can invoke JHAT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Epoch 1\n",
    "e1_cnt = len(epoch1_files)\n",
    "for i, f in enumerate(epoch1_files):\n",
    "    curr = i+1\n",
    "    print(\"\\t***** Processing Epoch 1 File %s/%s: `%s` *****\" % (curr, e1_cnt, f))\n",
    "    align_and_create_region_files(f, jhat_epoch1_outsubdir, ref_catname, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Epoch 2\n",
    "e2_cnt = len(epoch2_files)\n",
    "for i, f in enumerate(epoch2_files):\n",
    "    curr = i+1\n",
    "    print(\"\\t***** Processing Epoch 2 File %s/%s: `%s` *****\" % (curr, e2_cnt, f))\n",
    "    align_and_create_region_files(f, jhat_epoch2_outsubdir, ref_catname, max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Section 2: Building Aligned Lvl3 Mosaics And Projecting Them to the Same Tangential Plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Inputs: \n",
    "* Aligned Lvl 2 Cals for each epoch\n",
    "   * F200W, F356W\n",
    "### Outputs:\n",
    "* 4 Mosiacs:\n",
    "   * F200W Epoch1 (Reference Image), F200W Epoch2 (Science Image)\n",
    "   * F356W Epoch1 (Reference Image), F356W Epoch2 (Science Image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### We need to make a choice on the parameters of the location of the tangent plane. \n",
    "\n",
    "* Consider:\n",
    "   * Where you want your central coordinate in RA/Dec space to be?\n",
    "   * Where do you want to map that coordinate within the data array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_epoch1_F356W_files = glob.glob(\"%s/*nrcblong_jhat.fits\" % (jhat_epoch1_outsubdir))\n",
    "aligned_epoch1_F200W_files = glob.glob(\"%s/*nrcb2_jhat.fits\" % (jhat_epoch1_outsubdir))\n",
    "\n",
    "aligned_epoch2_F356W_files = glob.glob(\"%s/*nrcblong_jhat.fits\" % (jhat_epoch2_outsubdir))\n",
    "aligned_epoch2_F200W_files = glob.glob(\"%s/*nrcb2_jhat.fits\" % (jhat_epoch2_outsubdir))\n",
    "\n",
    "# What will make up each of our mosaics?\n",
    "pprint.pprint(aligned_epoch1_F356W_files)\n",
    "pprint.pprint(aligned_epoch2_F200W_files)\n",
    "\n",
    "\n",
    "# Choose an appropriate projection\n",
    "sw_pix_scale = 30.0 / 1000 # milliarcseconds\n",
    "lw_pix_scale = 60.0 / 1000 # milliarcseconds\n",
    "\n",
    "# Positioned between both SNe\n",
    "CRVAL1 = 53.1359270\n",
    "CRVAL2 = -27.8174387\n",
    "\n",
    "delta_ra = 0.01 # degrees\n",
    "delta_dec = 0.01 # degrees\n",
    "tile_shape = np.array((2048, 2048))\n",
    "\n",
    "CRPIX1, CRPIX2 = tile_shape / 2\n",
    "rotation = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### OK, now we can invoke the JWST pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Image3Pipeline()\n",
    "\n",
    "ep1_output_dir = \"./epoch1_mosaic\"\n",
    "ep2_output_dir = \"./epoch2_mosaic\"\n",
    "os.makedirs(ep1_output_dir, exist_ok=True)\n",
    "os.makedirs(ep2_output_dir, exist_ok=True)\n",
    "\n",
    "# We are setting parameters to be simple. However, what could you do to improve the mosaicing?\n",
    "params = {\n",
    "            'assign_mtwcs':         {'skip': True},\n",
    "            'tweakreg':             {'skip': True},\n",
    "            'outlier_detection':    {'skip': True},\n",
    "            'source_catalog':       {'skip': True},\n",
    "            'skymatch':             {'skip': True}\n",
    "        }\n",
    "\n",
    "params['resample'] = {  \n",
    "                        'skip': False,\n",
    "                        'pixfrac': 1.,\n",
    "                        'kernel': 'square',\n",
    "                        'fillval': 'indef',\n",
    "                        'weight_type': 'ivm',\n",
    "                        'in_memory': True,\n",
    "                        'save_results': True\n",
    "                    }\n",
    "\n",
    "params['resample']['pixel_scale'] = sw_pix_scale  \n",
    "params['resample']['rotation'] = rotation\n",
    "params['resample']['output_shape'] = (int(tile_shape[0]), int(tile_shape[1]))\n",
    "\n",
    "params['resample']['crpix'] = [CRPIX1, CRPIX2]\n",
    "params['resample']['crval'] = [CRVAL1, CRVAL2]\n",
    "\n",
    "# Perform SW first\n",
    "f200w_epoch1_basefile = \"f200w_epoch1\"\n",
    "f200w_epoch2_basefile = \"f200w_epoch2\"\n",
    "\n",
    "pipe3.call([img for img in aligned_epoch1_F200W_files], \n",
    "           steps=params, \n",
    "           output_dir=ep1_output_dir, \n",
    "           output_file=f200w_epoch1_basefile, \n",
    "           save_results=True)\n",
    "\n",
    "pipe3.call([img for img in aligned_epoch2_F200W_files], \n",
    "           steps=params, \n",
    "           output_dir=ep2_output_dir, \n",
    "           output_file=f200w_epoch2_basefile, \n",
    "           save_results=True)\n",
    "\n",
    "# Perform LW next\n",
    "f356w_epoch1_basefile = \"f356w_epoch1\"\n",
    "f356w_epoch2_basefile = \"f356w_epoch2\"\n",
    "\n",
    "params['resample']['pixel_scale'] = lw_pix_scale  \n",
    "\n",
    "pipe3.call([img for img in aligned_epoch1_F356W_files], \n",
    "           steps=params, \n",
    "           output_dir=ep1_output_dir, \n",
    "           output_file=f356w_epoch1_basefile, \n",
    "           save_results=True)\n",
    "\n",
    "pipe3.call([img for img in aligned_epoch2_F356W_files], \n",
    "           steps=params, \n",
    "           output_dir=ep2_output_dir, \n",
    "           output_file=f356w_epoch2_basefile, \n",
    "           save_results=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Section 3: Image Subtraction Two Ways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Inputs: \n",
    "* Mosaics for Epoch 1 and Epoch 2 in both filters\n",
    "   * F200W, F356W\n",
    "### Outputs:\n",
    "* 2 Difference images via HOTPANTS:\n",
    "   * F200W Epoch2 - Epoch1\n",
    "   * F356W Epoch2 - Epoch1\n",
    "* 2 Difference images via straight differencing:\n",
    "   * F200W Epoch2 - Epoch1\n",
    "   * F356W Epoch2 - Epoch1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "**The following function uses [Docker](https://docs.docker.com/desktop/) to abstract away the need to install HOTPANTS**:<BR>\n",
    "Docker is a virtualization framework that allows one to build very light weight containers that can execute Linux processes. Think of it like a very light weight virtual machine, and with it you can \"version control\" you computational environment.<BR>\n",
    "\n",
    "**Note, there's also a utility function, `create_stamp_catalog`, which creates the second catalog we need for HOTPANTS**:<BR>\n",
    "This source catalog is used for HOTPANTS to sample the PSFs of both files to allow it to convolve them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function that wraps a HOTPANTS call.\n",
    "def run_hotpants(inst_name, filt_name, ref_input_file, sci_input_file, cat_file, curr_working_dir, diff_dir):\n",
    "\n",
    "    # Utility dictionary to store the FWHM of the detectors in pixels.\n",
    "    filters = {}\n",
    "    filters['NIRCAM'] = {'F070W': 0.987, 'F090W': 1.103, 'F115W': 1.298, 'F140M': 1.553, 'F150W2': 1.628,\n",
    "                         'F150W': 1.770,\n",
    "                         'F162M': 1.801, 'F164N': 1.494, 'F182M': 1.990, 'F187N': 2.060, 'F200W': 2.141, 'F210M': 2.304,\n",
    "                         'F212N': 2.341, 'F250M': 1.340, 'F277W': 1.444, 'F300M': 1.585, 'F322W2': 1.547,\n",
    "                         'F323N': 1.711,\n",
    "                         'F335M': 1.760, 'F356W': 1.830, 'F360M': 1.901, 'F405N': 2.165, 'F410M': 2.179, 'F430M': 2.300,\n",
    "                         'F444W': 2.302, 'F460M': 2.459, 'F466N': 2.507, 'F470N': 2.535, 'F480M': 2.574}\n",
    "    filters['NIRISS'] = {'F090W': 1.40, 'F115W': 1.40, 'F140M': 1.50, 'F150W': 1.50, 'F158M': 1.50, 'F200W': 1.50,\n",
    "                         'F277W': 1.50, 'F356W': 1.60, 'F380M': 1.70, 'F430M': 1.80, 'F444W': 1.80, 'F480M': 1.80}\n",
    "    filters['MIRI'] = {'F560W': 1.636, 'F770W': 2.187, 'F1000W': 2.888, 'F1130W': 3.318, 'F1280W': 3.713,\n",
    "                       'F1500W': 4.354, 'F1800W': 5.224, 'F2100W': 5.989, 'F2550W': 7.312}\n",
    "    filters['WFC3'] = {'F105W': 1.001, 'F110W': 1.019, 'F125W': 1.053, 'F140W': 1.100, 'F160W': 1.176}\n",
    "    filters['ACS'] = {'F814W': 1.0}\n",
    "\n",
    "    # Utility function to take multiextension FITS files and save out each extension as its own file. \n",
    "    # HOTPANTS requires single extension files.\n",
    "    def unpack_fits(input_file, sci_out_filename, err_out_filename):\n",
    "        \n",
    "        # Unpack data into single extension files\n",
    "        with fits.open(input_file, output_verify='fix') as dat:\n",
    "    \n",
    "            sci_data = dat['SCI', 1].data\n",
    "            max_sci_val = np.nanmax(sci_data)\n",
    "        \n",
    "            err_data = dat['ERR', 1].data\n",
    "            err_median = np.nanmedian(err_data)\n",
    "            sci_pedastal = 50.0 * err_median\n",
    "            print(\"\\n\\tMedian error value: `%0.4f`; Pedastal to add to Sci Im: `%0.4f`\" % (err_median, sci_pedastal))\n",
    "        \n",
    "            dat[0].data = sci_data + sci_pedastal\n",
    "            dat[0].header = dat[0].header + dat[1].header\n",
    "            dat[0].writeto(sci_out_filename, overwrite=True)\n",
    "        \n",
    "            dat[0].data = err_data\n",
    "            dat[0].writeto(err_out_filename, overwrite=True)\n",
    "    \n",
    "            return max_sci_val\n",
    "    \n",
    "    # Utility function to mask out regions with NaNs\n",
    "    def create_mask(input_err_file, output_mask_file):\n",
    "\n",
    "        with fits.open(input_err_file, output_verify='fix') as dat:\n",
    "            sci_mask = np.zeros_like(dat[0].data)\n",
    "            sci_mask[dat[0].data == 0] = 0x80\n",
    "            sci_mask[np.isnan(dat[0].data)] = 0x80\n",
    "        \n",
    "            dat[0].data = sci_mask\n",
    "            dat[0].scale('int16')\n",
    "            dat.writeto(output_mask_file, overwrite=True)\n",
    "\n",
    "    # Takes Tabular Fits file w/ specific columns... feel free to generalize\n",
    "    def create_stamp_catalog(reference_cat_fits, input_file, filt_name, flux_thresh, stamp_outfile):\n",
    "        \n",
    "        # Build stampxy file\n",
    "        with fits.open(reference_cat_fits, memmap=True) as cat_file:\n",
    "        \n",
    "            table = Table(cat_file[\"CIRC_BSUB\"].data)            \n",
    "            \n",
    "            flux_key = \"{filt}_CIRC0\".format(filt=filt_name)\n",
    "            err_key = \"{filt}_CIRC0_e\".format(filt=filt_name)\n",
    "            \n",
    "            ras = table[\"RA\"]\n",
    "            decs = table[\"DEC\"]\n",
    "        \n",
    "            flux = table[flux_key]\n",
    "            flux_err = table[err_key]\n",
    "\n",
    "            # reverse sort so brightest sources first\n",
    "            indices = (-flux).argsort()\n",
    "        \n",
    "            sorted_ra = ras[indices]\n",
    "            sorted_dec = decs[indices]\n",
    "            sorted_fluxes = flux[indices]\n",
    "            sorted_flux_err = flux_err[indices]\n",
    "\n",
    "            # get sources brighter than some threshold\n",
    "            bright_ind = np.where(sorted_fluxes > flux_thresh)[0]\n",
    "            bright_ra = sorted_ra[bright_ind]\n",
    "            bright_dec = sorted_dec[bright_ind]\n",
    "            bright_flux = sorted_fluxes[bright_ind]\n",
    "            bright_flux_err = sorted_flux_err[bright_ind]\n",
    "                 \n",
    "            coords = SkyCoord(bright_ra, bright_dec, unit=(u.deg, u.deg))\n",
    "            sci_obs3 = space_phot.observation3(input_file)\n",
    "            y_max, x_max = sci_obs3.data.shape\n",
    "            xs, ys = sci_obs3.wcs.world_to_pixel(coords)\n",
    "            in_img_indices = np.where((xs >= 0) & (xs <= x_max) & (ys >= 0) & (ys <= y_max))[0]\n",
    "            \n",
    "            output_tbl = Table(\n",
    "                [xs[in_img_indices],\n",
    "                 ys[in_img_indices],\n",
    "                 bright_ra[in_img_indices],\n",
    "                 bright_dec[in_img_indices],\n",
    "                 bright_flux[in_img_indices]], names=[\"X\", \"Y\", \"RA\", \"DEC\", filt_name])\n",
    "        \n",
    "            output_tbl.write(stamp_outfile, format=\"ascii\", overwrite=True)\n",
    "            create_pixregionfile(xs[in_img_indices], ys[in_img_indices], stamp_outfile.replace(\"txt\", \"reg\"), color=\"red\",\n",
    "                                 coords=\"image\", radius=[0.4] * len(ys[in_img_indices]))\n",
    "\n",
    "    \n",
    "    # Create single extension file names for Science and Reference images\n",
    "    sci_output_file = sci_input_file.replace(\".fits\", \"_1.fits\")\n",
    "    sci_output_err_file = sci_input_file.replace(\".fits\", \"_1.noise.fits\")\n",
    "    sci_output_mask_file = sci_input_file.replace(\".fits\", \"_1.mask.fits\")\n",
    "    sci_stampxy_file = sci_input_file.replace('.fits', '.stampxy.txt')\n",
    "    max_val_sci_im = unpack_fits(sci_input_file, sci_output_file, sci_output_err_file)\n",
    "\n",
    "    ref_output_file = ref_input_file.replace(\".fits\", \"_1.fits\")\n",
    "    ref_output_err_file = ref_input_file.replace(\".fits\", \"_1.noise.fits\")\n",
    "    ref_output_mask_file = ref_input_file.replace(\".fits\", \"_1.mask.fits\")\n",
    "    max_val_ref_im = unpack_fits(ref_input_file, ref_output_file, ref_output_err_file)\n",
    "\n",
    "    # Create masks for Science and Reference images\n",
    "    create_mask(sci_output_err_file, sci_output_mask_file)\n",
    "    create_mask(ref_output_err_file, ref_output_mask_file)\n",
    "\n",
    "    # Dymanically create the Difference Image file name\n",
    "    diff_file_basename_formatter = \"{filt_name}_{sci_epoch}_{ref_epoch}.diff.fits\"\n",
    "    epoch_regex = r'epoch\\d+'\n",
    "\n",
    "    # For convenience, I am assuming that \"epoch\" is in the file names... you would need to generalize if you have a different\n",
    "    # naming convention\n",
    "    match = re.search(epoch_regex, ref_input_file)\n",
    "    ref_file_epoch = match.group(0)\n",
    "    \n",
    "    match = re.search(epoch_regex, sci_input_file)\n",
    "    sci_file_epoch = match.group(0)\n",
    "\n",
    "    # Set up the named files to capture HOTPANTS output\n",
    "    diff_file = os.path.join(diff_dir, \n",
    "                             diff_file_basename_formatter.format(filt_name=filt_name, sci_epoch=sci_file_epoch, ref_epoch=ref_file_epoch))\n",
    "    diff_err_file = diff_file.replace('.fits', '.noise.fits')\n",
    "    diff_mask_file = diff_file.replace('.fits', '.mask.fits')\n",
    "    diff_kernel_file = diff_file.replace('.fits', '.kernel.fits') \n",
    "    diff_stampxy_reg_file = diff_file.replace('.fits', '.stampxy.reg')\n",
    "\n",
    "    # Build the catalog for HOTPANTS to sample the PSFs\n",
    "    create_stamp_catalog(cat_file, sci_output_file, filt_name, flux_thresh=10.0, stamp_outfile=sci_stampxy_file)\n",
    "\n",
    "    # A bit dodgey: We're assuming the same instrument and filter between science and template image\n",
    "    sci_fwhm = 1.8\n",
    "    if inst_name in filters:\n",
    "        if filt_name in filters[inst_name]:\n",
    "            sci_fwhm = filters[inst_name][filt_name]\n",
    "    else:\n",
    "        print(f'{inst_name} not found in {list(filters.keys())}, FWHM fixed to 1.8 pixels.')\n",
    "\n",
    "    temp_fwhm = 1.8\n",
    "    if inst_name in filters:\n",
    "        if filt_name in filters[inst_name]:\n",
    "            temp_fwhm = filters[inst_name][filt_name]\n",
    "    else:\n",
    "        print(f'{inst_name} not found in {list(filters.keys())}, FWHM fixed to 1.8 pixels.')\n",
    "\n",
    "\n",
    "    # hotpants args\n",
    "    ko = 2  # spatial order of kernel variation within region (2)\n",
    "    bgo = 2  # spatial order of background variation within region (1)\n",
    "    ssig = 3.0  # threshold for sigma clipping statistics  (3.0)\n",
    "    \n",
    "    ks = 2.0  # high sigma rejection for bad stamps in kernel fit (2.0)\n",
    "    r = 2.5 * sci_fwhm  # FWHM * 2.5         # convolution kernel half width (10)\n",
    "    kfm = 0.99  # fraction of abs(kernel) sum for ok pixel (0.990)\n",
    "    \n",
    "    il = 0  # lower valid data count, image (0)\n",
    "    iu = max_val_sci_im  # dynamically take the highest in the science img        # upper valid data count, image (25000)\n",
    "    iuk = iu  # set to 'iu' for now     # upper valid data count for kernel, image (iuthresh)\n",
    "    \n",
    "    tl = 0  # lower valid data count, template (0)\n",
    "    tu = max_val_ref_im  # upper valid data count, template (25000)\n",
    "    tuk = tu  # upper valid data count for kernel, template (tuthresh)\n",
    "    \n",
    "    nrx = 1 #1  # number of image regions in x dimension (1)\n",
    "    nry = 1 #1  # number of image regions in y dimension (1)\n",
    "    \n",
    "    nsx = 8 #70  # number of each region's stamps in x dimension (10)\n",
    "    nsy = 8 #90  # number of each region's stamps in y dimension (10)\n",
    "    nss = 7  # number of centroids to use for each stamp (3) # DC: got a segmentation fault using \"10\"\n",
    "    \n",
    "    # ngauss degree0 sigma0 .. degreeN sigmaN]\n",
    "    # : ngauss = number of gaussians which compose kernel (3)\n",
    "    # : degree = degree of polynomial associated with gaussian #\n",
    "    #            (6 4 2)\n",
    "    # : sigma  = width of gaussian #\n",
    "    #            (0.70 1.50 3.00)\n",
    "    # : N = 0 .. ngauss - 1\n",
    "    \n",
    "    # : (3 6 0.70 4 1.50 2 3.00\n",
    "    ng = (3, 6, sci_fwhm / 2.0, 4, sci_fwhm, 2, sci_fwhm * 2.0)\n",
    "    \n",
    "    rss = 2.5 * sci_fwhm  # 2.5 * FWHM scaled by the fwhm - radius of the substamp # half width substamp to extract around each centroid (15)\n",
    "    ft = 5.0  # RMS threshold for good centroid in kernel fit (20.0)\n",
    "    \n",
    "    # -okn          # rescale noise for 'ok' pixels (0)\n",
    "    c = \"t\"  # force convolution on (t)emplate or (i)mage (undef)\n",
    "    n = \"i\"  # normalize to (t)emplate, (i)mage, or (u)nconvolved (t)\n",
    "    # -sconv        # all regions convolved in same direction (0)\n",
    "    \n",
    "    afssc = 0  # autofind stamp centers so #=-nss when -ssf,-cmp (1)\n",
    "    gridssc = 0\n",
    "    \n",
    "    fi = 0.0  # value for invalid (bad) pixels (1.0e-30)\n",
    "    fin = 0.0  # noise image only fillvalue (0.0e+00)\n",
    "    \n",
    "    mins = 2.0  # Fraction of kernel half width to spread input mask (1.0)\n",
    "    mous = 0.0  # Ditto output mask, negative = no diffim masking (1.0)\n",
    "    \n",
    "    v = 2  # level of verbosity, 0-2 (1)\n",
    "    # -savexy = set file name . saves the X,Y positions of used, clipped, and all substamps in different colors\n",
    "    # save as a reg file\n",
    "    \n",
    "    hotpants_arg = 'hotpants -inim {sci_file} -tmplim {temp_file} -outim {diff_file} -ini {sci_noise_im} \\\n",
    "                   -imi {sci_mask} -il {il} -iu {iu} -iuk {iuk} -tni {temp_noise_im} -tmi {temp_mask} \\\n",
    "                   -tl {tl} -tu {tu} -tuk {tuk} -nrx {nrx} -nry {nry} -nsx {nsx} -nsy {nsy} -nss {nss} -ng {ng_tuple} \\\n",
    "                   -rss {rss} -ft {ft} -r {r} -ko {ko} -bgo {bgo} -ssig {ssig} -ks {ks} -kfm {kfm} -okn -c {c} -n {n} -sconv \\\n",
    "                   -cmp {diff_substamps} -afssc {afssc} -gridssc {gridssc} -fi {fi} \\\n",
    "                   -oni {diff_noise_im}  -fin {fin} -mins {mins} \\\n",
    "                   -omi {diff_mask} -mous {mous} -oki {diff_kernel} -v {v} -savexy {savexy}'.format(\n",
    "        sci_file=sci_output_file, temp_file=ref_output_file, diff_file=diff_file,\n",
    "        sci_noise_im=sci_output_err_file, sci_mask=sci_output_mask_file, il=il, iu=iu, iuk=iuk, temp_noise_im=ref_output_err_file,\n",
    "        temp_mask=ref_output_mask_file, tl=tl, tu=tu, tuk=tuk, nrx=nrx, nry=nry, nsx=nsx, nsy=nsy, nss=nss,\n",
    "        ng_tuple=\" \".join([str(_ng) for _ng in ng]),\n",
    "        rss=rss, ft=ft, r=r, ko=ko, bgo=bgo, ssig=ssig, ks=ks, kfm=kfm, c=c, n=n, diff_substamps=sci_stampxy_file,\n",
    "        afssc=afssc, gridssc=gridssc, fi=fi, diff_noise_im=diff_err_file, fin=fin,\n",
    "        mins=mins, diff_mask=diff_mask_file, mous=mous, diff_kernel=diff_kernel_file, v=v, savexy=diff_stampxy_reg_file)\n",
    "    \n",
    "    print(\"Hotpants invocation:\\n\\t%s\" % hotpants_arg)\n",
    "\n",
    "    # This is where you need Docker!\n",
    "    os.system(\"docker run --rm -v %s:/app ghcr.io/davecoulter/diffpype:1.0.3 %s\" % (curr_working_dir, hotpants_arg))\n",
    "\n",
    "    return diff_file\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src='./assets/hotpants.png' width=\"100px\"/><h2>OK, now we can run HOTPANTS!</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to map our current working path to the docker container to get output locally\n",
    "local_working_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output dir\n",
    "diff_outsubdir = \"./diff\"\n",
    "os.makedirs(diff_outsubdir, exist_ok=True)\n",
    "\n",
    "f200w_ref = \"%s/%s_i2d.fits\" % (ep1_output_dir, f200w_epoch1_basefile)\n",
    "f200w_sci = \"%s/%s_i2d.fits\" % (ep2_output_dir, f200w_epoch2_basefile)\n",
    "\n",
    "f200w_diff_im = run_hotpants(inst_name=\"NIRCAM\", \n",
    "             filt_name=\"F200W\", \n",
    "             ref_input_file=f200w_ref, \n",
    "             sci_input_file=f200w_sci, \n",
    "             cat_file=ref_fits, \n",
    "             curr_working_dir=local_working_dir,\n",
    "             diff_dir=diff_outsubdir)\n",
    "\n",
    "\n",
    "f356w_ref = \"%s/%s_i2d.fits\" % (ep1_output_dir, f356w_epoch1_basefile)\n",
    "f356w_sci = \"%s/%s_i2d.fits\" % (ep2_output_dir, f356w_epoch2_basefile)\n",
    "\n",
    "f356w_diff_im = run_hotpants(inst_name=\"NIRCAM\", \n",
    "             filt_name=\"F356W\", \n",
    "             ref_input_file=f356w_ref, \n",
    "             sci_input_file=f356w_sci, \n",
    "             cat_file=ref_fits, \n",
    "             curr_working_dir=local_working_dir,\n",
    "             diff_dir=diff_outsubdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src='./assets/haha.png' width=\"250px\"/>\n",
    "<b>Now let's do an alternative to HOTPANTS:</b><BR>\n",
    "Because the PSF of JWST is so stable, it is possible to simple subtract the data arrays from each other, and estimating a resulting error image by adding the component error arrays in quadrature.<BR>\n",
    "\n",
    "**This works for JWST but won't for everything!**<BR>\n",
    "Can you think of some situations where the below approach would not work? Consider long baseline epochs, different position angles, different telescopes, ground based data, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Straight Diff for F200W\n",
    "straight_sci_f200w = fits.open(f200w_sci)\n",
    "straight_ref_f200w = fits.open(f200w_ref)\n",
    "straight_diff_f200w_img_ouput = f200w_diff_im.replace(\".diff.fits\", \".straight_diff.fits\")\n",
    "straight_diff_f200w_noise_ouput = straight_diff_f200w_img_ouput.replace(\".fits\", \".noise.fits\")\n",
    "\n",
    "straight_sci_f200w['SCI'].data -= straight_ref_f200w['SCI'].data\n",
    "# Just using the `straight_sci_f200w` in-memory FITS file to write out to a new file\n",
    "straight_sci_f200w.writeto(straight_diff_f200w_img_ouput, overwrite=True)\n",
    "\n",
    "straight_sci_f200w['SCI'].data = np.sqrt(straight_ref_f200w['ERR'].data**2 + straight_sci_f200w['ERR'].data**2)\n",
    "# Just using the `straight_sci_f200w` in-memory FITS file to write out to a new file\n",
    "straight_sci_f200w.writeto(straight_diff_f200w_noise_ouput, overwrite=True)\n",
    "\n",
    "# Run Straight Diff for F356W\n",
    "straight_sci_f356w = fits.open(f356w_sci)\n",
    "straight_ref_f356w = fits.open(f356w_ref)\n",
    "straight_diff_f356w_img_ouput = f356w_diff_im.replace(\".diff.fits\", \".straight_diff.fits\")\n",
    "straight_diff_f356w_noise_ouput = straight_diff_f356w_img_ouput.replace(\".fits\", \".noise.fits\")\n",
    "\n",
    "straight_sci_f356w['SCI'].data -= straight_ref_f356w['SCI'].data\n",
    "# Just using the `straight_sci_f356w` in-memory FITS file to write out to a new file\n",
    "straight_sci_f356w.writeto(straight_diff_f356w_img_ouput, overwrite=True)\n",
    "\n",
    "straight_sci_f356w['SCI'].data = np.sqrt(straight_ref_f356w['ERR'].data**2 + straight_sci_f356w['ERR'].data**2)\n",
    "# Just using the `straight_sci_f356w` in-memory FITS file to write out to a new file\n",
    "straight_sci_f356w.writeto(straight_diff_f356w_noise_ouput, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src='./assets/clippy.jpg' width=\"100px\"/><b>Let's check our handiwork:</b>\n",
    "* Open our Epoch1, Epoch2, and HOTPANTS Diff Image, and \"Straight Diff\" images\n",
    "* Look at the noise properties (DS9 region statistics)\n",
    "   * We want as little structure in the noise as possible\n",
    "   * Zoom into a galaxy -- at best, complex structures should be removed with little residuals\n",
    "   * Can you see any new sources by eye?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Section 4: Source Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Inputs: \n",
    "* Difference images for both filters\n",
    "   * F200W, F356W\n",
    "### Outputs:\n",
    "* 2 source catalogs with plottable regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Let's use a Python wrapper for Source Extractor\n",
    "\n",
    "* Note:\n",
    "   * This is not necessarily an optimum usage. What other parameters can we pass to Source Extractor?\n",
    "   * Similarly, what other parameters can we measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to wrap source extraction\n",
    "def run_sextractor_on_diff(diff_file, output_dir, snr_thresh):\n",
    "    \n",
    "    # class_star_cut = 0 # You can use this if you have a good stellar model\n",
    "    sew = sewpy.SEW(params=[\"X_IMAGE\", \"Y_IMAGE\", \"FLUX_RADIUS(3)\", \"FLAGS\", \"XPEAK_WORLD\", \"YPEAK_WORLD\",\"CLASS_STAR\"],\n",
    "                        config={\"DETECT_MINAREA\":3, \"PHOT_FLUXFRAC\":\"0.3, 0.5, 0.8\",'DETECT_THRESH':snr_thresh,\n",
    "                                'BACKPHOTO_TYPE':'local'}, loglevel=0)\n",
    "    \n",
    "    diff_wcs = WCS(fits.open(diff_file)['SCI'])\n",
    "    \n",
    "    found = sew(diff_file)['table']\n",
    "    found['xcentroid'] = found['X_IMAGE'] -1 # zero based to one based array fix\n",
    "    found['ycentroid'] = found['Y_IMAGE'] -1\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    found['skycoord_peak'] = diff_wcs.pixel_to_world(found['xcentroid'],found['ycentroid'])\n",
    "    # found = found[found['CLASS_STAR']>class_star_cut]\n",
    "    \n",
    "    table_cols = ['source_id','ra','dec']\n",
    "    table_data = [\n",
    "        ['src_'+str(x) for x in np.arange(0,len(found),1)],\n",
    "        [x.ra.value for x in found['skycoord_peak']],\n",
    "        [x.dec.value for x in found['skycoord_peak']]\n",
    "    ]\n",
    "    \n",
    "    source_cat = Table(names=table_cols,dtype=[str]+[float]+[float])\n",
    "    \n",
    "    for data_row in zip(*table_data):\n",
    "        new_row = {}\n",
    "        for i, d in enumerate(data_row):\n",
    "            new_row[table_cols[i]] = d\n",
    "        source_cat.add_row(new_row)\n",
    "    \n",
    "    source_cat.write(diff_file.replace(\".fits\", \".sources.txt\"), format='ascii', overwrite=True)\n",
    "    create_pixregionfile(source_cat['ra'], source_cat['dec'], diff_file.replace(\".fits\", \".sources.reg\"), color=\"red\",\n",
    "                                     coords=\"icrs\", radius=[0.4] * len(source_cat))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Let's invoke Source Extractor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sewpy_snr_thresh = 3.0\n",
    "run_sextractor_on_diff(f200w_diff_im, diff_outsubdir, sewpy_snr_thresh)\n",
    "run_sextractor_on_diff(f356w_diff_im, diff_outsubdir, sewpy_snr_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src='./assets/clippy.jpg' width=\"100px\"/><b>Let's check our handiwork:</b>\n",
    "* Open our Epoch1, Epoch2, and HOTPANTS Diff Image, and \"Straight Diff\" images\n",
    "* Overplot region files to review any sources you've found\n",
    "* Plot the SNe.reg file to see if you recover the SNe we know are in the images\n",
    "* Can you find any other sources?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Questions to think about:\n",
    "\n",
    "* How can would you improve the source detection step to remove false positives? Remember what you have at your disposal: \n",
    "   * Masks, error arrays, and SExtractor source properties\n",
    "* Often times we only want to regard a source as real if it appears in more than one filter or epoch. How could you adapt this code to achieve that?\n",
    "* What can you do with ML vs algorithmic source detection? How would you create a training set?\n",
    "* If you were interested in \"high-z\" candidates, how would you screen for that? Remember you have a catalog of the field..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Let's Look at Examples of \"Good\" and \"Bad\" Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Good Sources:<br>\n",
    "\n",
    "<img style=\"float: center;\" src='./assets/GoodSource1.png' width=\"200px\"/>\n",
    "<img style=\"float: center;\" src='./assets/GoodSource2.png' width=\"200px\"/><br>\n",
    "\n",
    "* PSF-like shapes (not too sharp, gaussian profile)\n",
    "* Lack of dipoles (artifact caused by bad alignment)\n",
    "* Noise properties around source are uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Bad Sources:<br>\n",
    "\n",
    "<img style=\"float: center;\" src='./assets/BadSource_CoreResidual.png' width=\"200px\"/>\n",
    "<img style=\"float: center;\" src='./assets/BadSource_CR.png' width=\"200px\"/>\n",
    "<img style=\"float: center;\" src='./assets/BadSource_MaskingArtifact.png' width=\"200px\"/><br>\n",
    "\n",
    "* PSF-like shapes (too sharp, appear as hot pixels)\n",
    "* Black and white core residuals (artifact caused by subtracting bright and complicated cores from each other [e.g., galaxies, diffraction spikes, etc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
